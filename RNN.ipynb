{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code was based on the example Keras script for LTSM https://github.com/fchollet/keras/blob/master/examples/lstm_text_generation.py\n",
    "\n",
    "The key differences are\n",
    "- Generation is not based on a sequence from the input text, it's based on a random starting character.\n",
    "- Various possible hidden architectures\n",
    "- Save best models throughout training\n",
    "- Patience/early stopping while training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Various required inputs\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM, GRU\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import string\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prints the text generations to a file\n",
    "out_file = \"output/onelayer-full-gru.txt\"\n",
    "out = open(out_file, 'w')\n",
    "\n",
    "# Where to save the best weights\n",
    "weights_path = \"weights/gru_full_weights.h5\"\n",
    "\n",
    "# The file to use for training\n",
    "file_name = \"data/full.txt\"\n",
    "text = open(file_name).read().replace('\\r', '').replace('\\n', '').lower() # Do not worry about case sensitivity and remove new line chars\n",
    "\n",
    "# Extract unique set of chars from the text\n",
    "chars = sorted(list(set(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Mapping from char -> int and back\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# Split the text into max_length chunks\n",
    "max_length = 40\n",
    "step = 3\n",
    "sentences = [] # This will be treated as the input\n",
    "next_chars = [] # This will be treated as the output/prediction\n",
    "\n",
    "for i in range(0, len(text) - max_length, step):\n",
    "    sentences.append(text[i: i + max_length])\n",
    "    next_chars.append(text[i + max_length])\n",
    "\n",
    "# Vectorize the inputs and outputs\n",
    "x = np.zeros((len(sentences), max_length, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hidden layer(s) options\n",
    "def onelayer_LSTM():\n",
    "    model.add(LSTM(128, input_shape=(max_length, len(chars)), dropout=dropout_rate))\n",
    "\n",
    "def threelayer_LSTM():\n",
    "    model.add(LSTM(32, return_sequences=True, input_shape=(max_length, len(chars)), dropout=dropout_rate))\n",
    "    model.add(LSTM(32, return_sequences=True, dropout=dropout_rate))\n",
    "    model.add(LSTM(32, dropout=dropout_rate))\n",
    "\n",
    "def onelayer_GRU():\n",
    "    model.add(GRU(128, input_shape=(max_length, len(chars)), dropout=dropout_rate))\n",
    "\n",
    "def threelayer_GRU():\n",
    "    model.add(GRU(32, return_sequences=True, input_shape=(max_length, len(chars)), dropout=dropout_rate))\n",
    "    model.add(GRU(32, return_sequences=True, dropout=dropout_rate))\n",
    "    model.add(GRU(32, dropout=dropout_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build the model. Calll the appropriate function based on desired architecture\n",
    "model = Sequential()\n",
    "\n",
    "# Dropout rate for hidden layers. Set to zero for no dropout\n",
    "dropout_rate = 0.2\n",
    "\n",
    "# Swap line below to change hidden architecture\n",
    "#onelayer_LSTM()\n",
    "onelayer_GRU()\n",
    "#threelayer_LSTM()\n",
    "#threelayer_GRU()\n",
    "\n",
    "# Output layer, softmax probabilities for each character\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "#optimizer = RMSProp(lr=0.1)\n",
    "#optimizer = Adam(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# helper function to sample an index from a probability array\n",
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "def generate_text(model, diversity, start, length):\n",
    "    generated = start\n",
    "    print('----- Generating with starting token: \"' + generated + '\"')\n",
    "\n",
    "    for i in range(length):\n",
    "        # Use only the last max_length characters for prediction\n",
    "        recent_chars = generated[- max_length:]\n",
    "\n",
    "        x_pred = np.zeros((1, max_length, len(chars)))\n",
    "        for t, char in enumerate(recent_chars):\n",
    "            x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "        # Predict the next character in turn\n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        next_index = sample(preds, diversity)\n",
    "        # The next character generated/predicted\n",
    "        next_char = indices_char[next_index]\n",
    "        generated += next_char\n",
    "    \n",
    "    out.write(\"\\t\\t\"+generated+\"\\n\")\n",
    "    print(generated)\n",
    "    print()\n",
    "    \n",
    "def generate(model, diversity, length):\n",
    "    start = random.choice(string.ascii_lowercase)\n",
    "    generate_text(model, diversity, start, length)\n",
    "    \n",
    "def generate_diverse(model, length):\n",
    "    for diversity in [0.01, 0.2, 0.5, 1.0]:\n",
    "        print()\n",
    "        print('----- diversity:', diversity)\n",
    "        out.write(\"\\tDiversity: \" + str(diversity) +\"\\n\")\n",
    "\n",
    "        # Use a length of 140, match twitters limit so we can generate pretend tweets\n",
    "        generate(model, diversity, length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 500 # Number of epochs to run\n",
    "best_loss = float('inf') # Keep track of best loss, start at infinity\n",
    "patience = 20 # Number of iterations without progress\n",
    "no_improvement = 0 # Number of current iterations without improvement\n",
    "\n",
    "# Train the model, output generated text after each iteration. Be sure to give enough iterations to adequately learn (20+ minimum)\n",
    "for iteration in range(1, epochs + 1):\n",
    "    print()\n",
    "    print('-' * 50)\n",
    "    print('Iteration', iteration)\n",
    "    \n",
    "    # Only one epoch here, as the actual epochs are controlled by the iteration loop so we have additional control\n",
    "    hist = model.fit(x, y,\n",
    "              batch_size=128,\n",
    "              epochs=1)\n",
    "    \n",
    "    loss = hist.history['loss'][-1]\n",
    "        \n",
    "    if loss < best_loss:\n",
    "        best_loss = loss\n",
    "        model.save_weights(weights_path)\n",
    "        no_improvement = 0\n",
    "    else:\n",
    "        no_improvement += 1\n",
    "    \n",
    "    # If we havent improved for patience generations, then stop early \n",
    "    if no_improvement > patience:\n",
    "        # Could look at trying other things such as decreasing learning rate\n",
    "        break\n",
    "    \n",
    "    # Unlikely - but if we achieve a zero loss we are finished\n",
    "    if loss == 0:\n",
    "        break\n",
    "\n",
    "    # Print out some text generations every 10 iterations, so we can monitor how the model is progressing\n",
    "    if iteration % 10 == 0:\n",
    "        out.write(\"Iteration: \" + str(iteration) + \"\\n\")\n",
    "        \n",
    "        for i in range(5):\n",
    "            generate_diverse(model, 140)\n",
    "        \n",
    "        # Force the file write\n",
    "        out.flush()\n",
    "        os.fsync(out.fileno())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the model that had lowest loss\n",
    "model.load_weights(weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate some samples with this best model\n",
    "out.write(\"\\n----------------\\n\")\n",
    "out.write(\"Final Generations\\n\")\n",
    "\n",
    "for i in range(50):\n",
    "    generate_diverse(model, 140)\n",
    "        \n",
    "out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
